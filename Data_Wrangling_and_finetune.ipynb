{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1:\n",
        "## 1. Data Wrangling & EDA\n"
      ],
      "metadata": {
        "id": "dBsCt5U_r85o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6nDwIBiDsZBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2. Install Required Libraries"
      ],
      "metadata": {
        "id": "UowzdkEOsLga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pandas matplotlib"
      ],
      "metadata": {
        "id": "XQFbFiIisOLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Load the Amazon Reviews Data"
      ],
      "metadata": {
        "id": "5IkNmrmessF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load a subset (e.g., 10,000 rows for speed)\n",
        "ds = load_dataset(\"amazon_polarity\", split=\"train[:10000]\")\n",
        "df = pd.DataFrame(ds)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "loqteNuEsvk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label'].value_counts())\n"
      ],
      "metadata": {
        "id": "-RzEDXmJs0WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "hK7aA06bxFLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['label', 'title', 'content']].sample(5)\n"
      ],
      "metadata": {
        "id": "obOu9Y_cxY4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'] = df['content'].apply(lambda x: len(x.split()))\n",
        "df[['word_count']].describe()\n"
      ],
      "metadata": {
        "id": "8JSTvxgWxcM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df['word_count'].hist(bins=30, edgecolor='black')\n",
        "plt.xlabel('Number of Words per Review')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.title('Review Length Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Un0SKmacxmWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_words = ' '.join(df['content']).lower().split()\n",
        "common_words = Counter(all_words).most_common(20)\n",
        "print(\"Top 20 words:\", common_words)\n"
      ],
      "metadata": {
        "id": "YKs12QsexqyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"amazon_reviews_raw.csv\", index=False)\n",
        "from google.colab import files\n",
        "files.download(\"amazon_reviews_raw.csv\")\n"
      ],
      "metadata": {
        "id": "oH5i3Fwex5nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_content'] = df['content'].str.lower()\n"
      ],
      "metadata": {
        "id": "jZJRnYIsx_9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "df['clean_content'] = df['clean_content'].str.translate(str.maketrans('', '', string.punctuation))\n"
      ],
      "metadata": {
        "id": "thDy3YrI06IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_content'] = df['clean_content'].str.replace(r'\\d+', '', regex=True)\n"
      ],
      "metadata": {
        "id": "WqJ1esco08Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_content'] = df['clean_content'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"
      ],
      "metadata": {
        "id": "nX_B2xeR09rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "2r1bETGr0_Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "df['clean_content'] = df['clean_content'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "PdKENjPl1BLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "Z2Ugt8ma1FNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# For demo speed, do just first 1,000 rows\n",
        "df.loc[:999, 'clean_content'] = df.loc[:999, 'clean_content'].apply(lemmatize)\n"
      ],
      "metadata": {
        "id": "Xbtc2MDv1FyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['clean_content'].str.split().str.len() > 3]\n",
        "print(\"After removing short reviews:\", df.shape)\n"
      ],
      "metadata": {
        "id": "M0vZGnn51MGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['content', 'clean_content']].sample(5)\n"
      ],
      "metadata": {
        "id": "jwlLkbyg1NFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"amazon_reviews_cleaned.csv\", index=False)\n",
        "from google.colab import files\n",
        "files.download(\"amazon_reviews_cleaned.csv\")\n"
      ],
      "metadata": {
        "id": "8G50CXd41PYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['clean_content']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "zM7_9cBL1Uu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "FzGIPM1k18x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=200)\n",
        "clf.fit(X_train_vec, y_train)\n"
      ],
      "metadata": {
        "id": "CKyX6nid1-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "BkTGD_hU2Akw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(clf, \"logreg_model.joblib\")\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
        "from google.colab import files\n",
        "files.download(\"logreg_model.joblib\")\n",
        "files.download(\"tfidf_vectorizer.joblib\")\n"
      ],
      "metadata": {
        "id": "oi7Fi0kt2CMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hEO92Skj2LR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n"
      ],
      "metadata": {
        "id": "B_EDUnsb2clr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "7iFePOap0Bdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/amazon_reviews_cleaned.csv')\n",
        "df = df.dropna(subset=['clean_content', 'label'])  # Just in case"
      ],
      "metadata": {
        "id": "BXYytiMzxsAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "hYktls2GyqCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'clean_content': 'text'})\n",
        "df = df[['text', 'label']]\n",
        "df = df[df['text'].str.strip().astype(bool)]  # Remove empty rows\n"
      ],
      "metadata": {
        "id": "U1hCuxmW01ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ZgS5mItQ2LMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['text'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Ca969P0w2M61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased\"  # Or \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Tokenize (batched for speed)\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "yS1lF3Oh3XI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
        "val_dataset = ReviewDataset(val_encodings, val_labels)\n"
      ],
      "metadata": {
        "id": "M8HGHzj03l8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "iYzFNd2AA1pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# OPTIONAL: Use only a small subset for quick testing (e.g., first 2000 samples)\n",
        "small_train_dataset = torch.utils.data.Subset(train_dataset, range(2000))\n",
        "small_val_dataset = torch.utils.data.Subset(val_dataset, range(500))\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,  # Only 1 epoch for speed\n",
        "    per_device_train_batch_size=16,  # Bigger batch = faster (if you have GPU memory)\n",
        "    per_device_eval_batch_size=32,   # Same for eval\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",  # Don't save checkpoints\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,   # Log less frequently\n",
        "    report_to=[],        # Turn off all reporting (no wandb, no tensorboard)\n",
        "    disable_tqdm=False,  # Progress bar (set True if it slows down Colab)\n",
        "    fp16=True if torch.cuda.is_available() else False,  # Mixed precision on GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,  # Use subset\n",
        "    eval_dataset=small_val_dataset,     # Use subset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "3yLp59h49ziP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "ZmrPa9BZ94vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = trainer.predict(val_dataset)\n",
        "y_pred = np.argmax(preds.predictions, axis=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(val_labels, y_pred))\n",
        "print(confusion_matrix(val_labels, y_pred))\n"
      ],
      "metadata": {
        "id": "faPsjN3NMcJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./finetuned_distilbert\")\n",
        "tokenizer.save_pretrained(\"./finetuned_distilbert\")\n"
      ],
      "metadata": {
        "id": "cLApnXyqM28O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}